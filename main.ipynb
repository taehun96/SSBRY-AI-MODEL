{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ccc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6feef7",
   "metadata": {},
   "source": [
    "### 1단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49c48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "설치 확인 중...\n",
      "==================================================\n",
      "PyTorch: 2.5.1\n",
      "Device: CPU\n",
      "OpenCV: 4.12.0\n",
      "\n",
      "YOLO 모델 테스트 중...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 2.1MB/s 3.0s.9s<0.1s5sss\n",
      "YOLO 모델 로드 성공!\n",
      "\n",
      "0: 640x640 (no detections), 232.0ms\n",
      "Speed: 23.0ms preprocess, 232.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "모델 추론 테스트 성공!\n",
      "\n",
      "모든 설치가 완료되었습니다!\n",
      "다음: python step2_augmented_labeling.py 실행\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 설치 확인\n",
    "\n",
    "def check_installation():\n",
    "    \"\"\"설치된 패키지들 확인\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"설치 확인 중...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # PyTorch 확인\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    \n",
    "    # OpenCV 확인\n",
    "    print(f\"OpenCV: {cv.__version__}\")\n",
    "    \n",
    "    print(\"\\nYOLO 모델 테스트 중...\")\n",
    "    \n",
    "    # YOLO 모델 로드 테스트\n",
    "    try:\n",
    "        model = YOLO('yolov8n.pt')\n",
    "        print(\"YOLO 모델 로드 성공!\")\n",
    "        \n",
    "        # 더미 이미지로 추론 테스트\n",
    "        dummy_image = np.zeros((640, 640, 3), dtype=np.uint8)\n",
    "        results = model(dummy_image)\n",
    "        print(\"모델 추론 테스트 성공!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"YOLO 모델 테스트 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if check_installation():\n",
    "        print(\"\\n모든 설치가 완료되었습니다!\")\n",
    "        print(\"다음: python step2_augmented_labeling.py 실행\")\n",
    "    else:\n",
    "        print(\"\\n설치에 문제가 있습니다. 에러 메시지를 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75391f96",
   "metadata": {},
   "source": [
    "### 2단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4584f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 증강 포함 데이터셋 생성\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "이미지 수집 중...\n",
      "  can: 359장\n",
      "  glass: 504장\n",
      "  paper: 605장\n",
      "  plastic: 466장\n",
      "  trash: 82장\n",
      "  vinyl: 119장\n",
      "\n",
      "총 2135장의 원본 이미지\n",
      "\n",
      "데이터 처리 중...\n",
      "  증강 옵션: ON (원본 + 어두운/밝은 버전 생성)\n",
      "  진행: 50/2135 (150개 생성)\n",
      "  진행: 100/2135 (300개 생성)\n",
      "  진행: 150/2135 (450개 생성)\n",
      "  진행: 200/2135 (600개 생성)\n",
      "  진행: 250/2135 (750개 생성)\n",
      "  진행: 300/2135 (900개 생성)\n",
      "  진행: 350/2135 (1050개 생성)\n",
      "  진행: 400/2135 (1200개 생성)\n",
      "  진행: 450/2135 (1350개 생성)\n",
      "  진행: 500/2135 (1500개 생성)\n",
      "  진행: 550/2135 (1650개 생성)\n",
      "  진행: 600/2135 (1800개 생성)\n",
      "  진행: 650/2135 (1950개 생성)\n",
      "  진행: 700/2135 (2100개 생성)\n",
      "  진행: 750/2135 (2250개 생성)\n",
      "  진행: 800/2135 (2400개 생성)\n",
      "  진행: 850/2135 (2550개 생성)\n",
      "  진행: 900/2135 (2700개 생성)\n",
      "  진행: 950/2135 (2850개 생성)\n",
      "  진행: 1000/2135 (3000개 생성)\n",
      "  진행: 1050/2135 (3150개 생성)\n",
      "  진행: 1100/2135 (3300개 생성)\n",
      "  진행: 1150/2135 (3450개 생성)\n",
      "  진행: 1200/2135 (3600개 생성)\n",
      "  진행: 1250/2135 (3750개 생성)\n",
      "  진행: 1300/2135 (3900개 생성)\n",
      "  진행: 1350/2135 (4050개 생성)\n",
      "  진행: 1400/2135 (4200개 생성)\n",
      "  진행: 1450/2135 (4350개 생성)\n",
      "  진행: 1500/2135 (4500개 생성)\n",
      "  진행: 1550/2135 (4650개 생성)\n",
      "  진행: 1600/2135 (4800개 생성)\n",
      "  진행: 1650/2135 (4950개 생성)\n",
      "  진행: 1700/2135 (5100개 생성)\n",
      "  진행: 1750/2135 (5166개 생성)\n",
      "  진행: 1800/2135 (5216개 생성)\n",
      "  진행: 1850/2135 (5266개 생성)\n",
      "  진행: 1900/2135 (5316개 생성)\n",
      "  진행: 1950/2135 (5366개 생성)\n",
      "  진행: 2000/2135 (5416개 생성)\n",
      "  진행: 2050/2135 (5466개 생성)\n",
      "  진행: 2100/2135 (5516개 생성)\n",
      "\n",
      "처리 완료: 5551개 이미지 생성\n",
      "  훈련: 5124장\n",
      "  검증: 427장\n",
      "\n",
      "dataset.yaml 생성 완료\n",
      "\n",
      "2단계 완료!\n",
      "다음: python step3_train.py 실행\n"
     ]
    }
   ],
   "source": [
    "# 2단계: 데이터 준비 (데이터 증강 포함)\n",
    "\n",
    "def detect_object_in_image(image):\n",
    "    \"\"\"이미지에서 객체 위치 자동 탐지\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blurred = cv.GaussianBlur(gray, (5, 5), 0)\n",
    "    _, binary = cv.threshold(blurred, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    \n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    cleaned = cv.morphologyEx(binary, cv.MORPH_CLOSE, kernel)\n",
    "    cleaned = cv.morphologyEx(cleaned, cv.MORPH_OPEN, kernel)\n",
    "    \n",
    "    contours, _ = cv.findContours(cleaned, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv.contourArea)\n",
    "        x, y, w, h = cv.boundingRect(largest_contour)\n",
    "        \n",
    "        if w * h < width * height * 0.01:\n",
    "            margin = 0.1\n",
    "            x, y = int(width * margin), int(height * margin)\n",
    "            w, h = int(width * 0.8), int(height * 0.8)\n",
    "        \n",
    "        center_x = (x + w / 2) / width\n",
    "        center_y = (y + h / 2) / height\n",
    "        norm_w = w / width\n",
    "        norm_h = h / height\n",
    "        \n",
    "        return center_x, center_y, norm_w, norm_h\n",
    "    else:\n",
    "        return 0.5, 0.5, 0.8, 0.8\n",
    "\n",
    "def augment_brightness(image):\n",
    "    \"\"\"밝기 변화 - 다양한 조명 환경 시뮬레이션\"\"\"\n",
    "    variations = []\n",
    "    \n",
    "    variations.append((\"original\", image))\n",
    "    variations.append((\"dark\", cv.convertScaleAbs(image, alpha=0.6, beta=0)))\n",
    "    variations.append((\"very_dark\", cv.convertScaleAbs(image, alpha=0.4, beta=0)))\n",
    "    variations.append((\"bright\", cv.convertScaleAbs(image, alpha=1.3, beta=10)))\n",
    "    variations.append((\"low_contrast\", cv.convertScaleAbs(image, alpha=0.8, beta=0)))\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def add_noise(image, noise_level=10):\n",
    "    \"\"\"노이즈 추가 - 저조도 환경 시뮬레이션\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, image.shape).astype(np.int16)\n",
    "    noisy = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def setup_augmented_dataset(source_path=\"trashnet\", augment=True):\n",
    "    \"\"\"데이터 증강을 포함한 데이터셋 생성\"\"\"\n",
    "    source = Path(source_path)\n",
    "    target = Path(\"dataset\")\n",
    "    \n",
    "    if target.exists():\n",
    "        response = input(\"기존 dataset 폴더를 삭제하고 다시 만들까요? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            shutil.rmtree(target)\n",
    "        else:\n",
    "            print(\"취소됨\")\n",
    "            return False\n",
    "    \n",
    "    # 폴더 생성\n",
    "    for split in ['train', 'val']:\n",
    "        (target / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "        (target / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    classes = {'can': 0, 'glass': 1, 'paper': 2, 'plastic': 3, 'trash': 4, 'vinyl': 5}\n",
    "    \n",
    "    # 이미지 수집\n",
    "    all_files = []\n",
    "    print(\"\\n이미지 수집 중...\")\n",
    "    for class_name, class_id in classes.items():\n",
    "        class_dir = source / class_name\n",
    "        if class_dir.exists():\n",
    "            count = 0\n",
    "            for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                images = list(class_dir.glob(f\"*{ext}\"))\n",
    "                count += len(images)\n",
    "                for img_path in images:\n",
    "                    all_files.append((img_path, class_id, class_name))\n",
    "            print(f\"  {class_name}: {count}장\")\n",
    "    \n",
    "    if len(all_files) < 10:\n",
    "        print(\"이미지가 너무 적습니다 (최소 10장 필요)\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n총 {len(all_files)}장의 원본 이미지\")\n",
    "    \n",
    "    random.shuffle(all_files)\n",
    "    split_idx = int(len(all_files) * 0.8)\n",
    "    \n",
    "    processed = 0\n",
    "    \n",
    "    print(\"\\n데이터 처리 중...\")\n",
    "    if augment:\n",
    "        print(\"  증강 옵션: ON (원본 + 어두운/밝은 버전 생성)\")\n",
    "    else:\n",
    "        print(\"  증강 옵션: OFF (원본만)\")\n",
    "    \n",
    "    for i, (img_path, class_id, class_name) in enumerate(all_files):\n",
    "        try:\n",
    "            img = cv.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv.resize(img, (640, 640))\n",
    "            split = 'train' if i < split_idx else 'val'\n",
    "            \n",
    "            # 증강 적용\n",
    "            if augment and split == 'train':\n",
    "                variations = augment_brightness(img)\n",
    "                selected = [variations[0]]\n",
    "                if len(variations) > 1:\n",
    "                    selected.extend(random.sample(variations[1:], min(2, len(variations)-1)))\n",
    "            else:\n",
    "                selected = [(\"original\", img)]\n",
    "            \n",
    "            # 저장\n",
    "            for aug_type, aug_img in selected:\n",
    "                if \"dark\" in aug_type and random.random() > 0.5:\n",
    "                    aug_img = add_noise(aug_img, noise_level=15)\n",
    "                \n",
    "                cx, cy, w, h = detect_object_in_image(aug_img)\n",
    "                img_name = f\"{processed:05d}.jpg\"\n",
    "                \n",
    "                cv.imwrite(str(target / \"images\" / split / img_name), aug_img)\n",
    "                \n",
    "                with open(target / \"labels\" / split / f\"{processed:05d}.txt\", 'w') as f:\n",
    "                    f.write(f\"{class_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                \n",
    "                processed += 1\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"  진행: {i+1}/{len(all_files)} ({processed}개 생성)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n처리 완료: {processed}개 이미지 생성\")\n",
    "    \n",
    "    train_count = len(list((target / \"images\" / \"train\").glob(\"*.jpg\")))\n",
    "    val_count = len(list((target / \"images\" / \"val\").glob(\"*.jpg\")))\n",
    "    print(f\"  훈련: {train_count}장\")\n",
    "    print(f\"  검증: {val_count}장\")\n",
    "    \n",
    "    # dataset.yaml 생성\n",
    "    yaml_content = \"\"\"path: ./dataset\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: 6\n",
    "names: ['can', 'glass', 'paper', 'plastic', 'trash', 'vinyl']\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"dataset.yaml\", \"w\") as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    print(\"\\ndataset.yaml 생성 완료\")\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"데이터 증강 포함 데이터셋 생성\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    source = input(\"trashnet 폴더 경로 (엔터=trashnet): \").strip() or \"trashnet\"\n",
    "    use_augment = input(\"데이터 증강 사용? (y/n, 엔터=y): \").strip().lower() or 'y'\n",
    "    \n",
    "    if setup_augmented_dataset(source, use_augment == 'y'):\n",
    "        print(\"\\n2단계 완료!\")\n",
    "        print(\"다음: python step3_train.py 실행\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53d78d",
   "metadata": {},
   "source": [
    "### 3단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee169fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3단계: YOLO 쓰레기 분류 모델 훈련\n",
      "==================================================\n",
      "YOLO 모델 훈련 준비\n",
      "Device: cpu\n",
      "\n",
      "데이터셋 상태 확인 중...\n",
      "==================================================\n",
      "훈련 이미지: 5124개\n",
      "검증 이미지: 427개\n",
      "훈련 라벨: 5124개\n",
      "검증 라벨: 427개\n",
      "클래스 수: 6\n",
      "클래스: ['can', 'glass', 'paper', 'plastic', 'trash', 'vinyl']\n",
      "==================================================\n",
      "데이터셋 준비 완료!\n",
      "\n",
      "빠른 훈련 설정 (30 에포크)\n",
      "\n",
      "모델 훈련 시작!\n",
      "모델 크기: YOLOv8n\n",
      "에포크: 30\n",
      "배치 크기: 4\n",
      "이미지 크기: 416\n",
      "==================================================\n",
      "YOLOv8n 사전훈련 모델 로드 완료\n",
      "New https://pypi.org/project/ultralytics/8.3.204 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.203  Python-3.11.13 torch-2.5.1 CPU (Intel Core Ultra 7 258V)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=waste_classification, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\jk316\\project2\\runs\\detect\\waste_classification, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752482  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,018 parameters, 3,012,002 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 6.57.6 MB/s, size: 80.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jk316\\project2\\dataset\\labels\\train... 5124 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5124/5124 334.1it/s 15.3s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\jk316\\project2\\dataset\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 5.84.4 MB/s, size: 70.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jk316\\project2\\dataset\\labels\\val... 427 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 427/427 330.5it/s 1.3s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\jk316\\project2\\dataset\\labels\\val.cache\n",
      "Plotting labels to C:\\Users\\jk316\\project2\\runs\\detect\\waste_classification\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\jk316\\project2\\runs\\detect\\waste_classification\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30         0G     0.4549       2.15      1.143         14        416: 100% ━━━━━━━━━━━━ 1281/1281 2.9it/s 7:17<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.698       0.36      0.426      0.304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30         0G      0.398      1.573      1.096          7        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:55<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.2it/s 12.8s0.2s\n",
      "                   all        427        427      0.743      0.452      0.515      0.402\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30         0G     0.3763      1.456      1.086         10        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:54<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.456      0.544      0.534      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30         0G     0.3598      1.368      1.079          9        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.7s0.2s\n",
      "                   all        427        427      0.465      0.592      0.537      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30         0G     0.3595      1.335      1.082         10        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.2it/s 12.7s0.2s\n",
      "                   all        427        427      0.425      0.606      0.554      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30         0G     0.3287      1.275      1.068          9        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.549      0.639      0.604       0.48\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30         0G     0.3179      1.235      1.056         11        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.372      0.614      0.572       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30         0G     0.3133        1.2      1.061         11        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:50<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.681      0.552       0.59      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30         0G     0.3064      1.173      1.058          7        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:51<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.571       0.63      0.652      0.537\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30         0G     0.2968       1.14      1.053         11        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:51<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427        0.6      0.621      0.689      0.587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30         0G     0.2841      1.118      1.045         11        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:51<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.734      0.638      0.721      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/30         0G     0.2783      1.094      1.041         14        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.692      0.603      0.698      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/30         0G     0.2753      1.058      1.044         14        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.4s0.2s\n",
      "                   all        427        427      0.732      0.604      0.688      0.568\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/30         0G     0.2632      1.043      1.036         14        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.713      0.695      0.745      0.617\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/30         0G     0.2633      1.006      1.031         14        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427       0.75       0.71      0.751      0.624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/30         0G     0.2679     0.9832      1.036         14        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:54<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.758       0.71      0.768       0.64\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/30         0G     0.2491     0.9824      1.024         12        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.725      0.708      0.758      0.641\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/30         0G     0.2498     0.9521      1.026         11        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:52<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.741      0.676      0.767      0.652\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/30         0G     0.2452     0.9235      1.022         12        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:53<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.7s0.2s\n",
      "                   all        427        427      0.671      0.728      0.742      0.637\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/30         0G      0.239     0.9125      1.023          9        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:53<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.7s0.2s\n",
      "                   all        427        427      0.641      0.691      0.723      0.622\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/30         0G     0.3047     0.7216      1.123          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.7it/s 7:46<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.7s0.2s\n",
      "                   all        427        427      0.709      0.685      0.745      0.657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/30         0G      0.278     0.6524      1.101          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:43<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.738      0.713      0.781      0.684\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/30         0G     0.2701     0.5997      1.094          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:42<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.664      0.767      0.781      0.689\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/30         0G     0.2603     0.5586      1.084          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:43<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.688      0.765      0.789      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/30         0G     0.2572     0.5094      1.084          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:42<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.715      0.752        0.8      0.704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/30         0G     0.2424     0.4702      1.065          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:43<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.5s0.2s\n",
      "                   all        427        427      0.696      0.747      0.794      0.686\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/30         0G     0.2324     0.4357      1.057          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:42<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.714      0.782      0.799      0.699\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/30         0G     0.2189     0.4196      1.053          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:44<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.724      0.734       0.79      0.687\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/30         0G      0.216     0.3767      1.051          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:43<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.716      0.773      0.817      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/30         0G     0.2097     0.3607      1.044          4        416: 100% ━━━━━━━━━━━━ 1281/1281 2.8it/s 7:43<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 4.3it/s 12.6s0.2s\n",
      "                   all        427        427      0.727      0.785      0.828      0.728\n",
      "\n",
      "30 epochs completed in 4.007 hours.\n",
      "Optimizer stripped from C:\\Users\\jk316\\project2\\runs\\detect\\waste_classification\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\Users\\jk316\\project2\\runs\\detect\\waste_classification\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\Users\\jk316\\project2\\runs\\detect\\waste_classification\\weights\\best.pt...\n",
      "Ultralytics 8.3.203  Python-3.11.13 torch-2.5.1 CPU (Intel Core Ultra 7 258V)\n",
      "Model summary (fused): 72 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 5.0it/s 10.7s0.2s\n",
      "                   all        427        427      0.726      0.786      0.828      0.728\n",
      "                   can         68         68      0.759      0.832      0.899       0.87\n",
      "                 glass        109        109      0.878      0.855      0.901      0.768\n",
      "                 paper        117        117      0.921      0.801      0.923      0.765\n",
      "               plastic         90         90      0.823      0.671      0.805      0.669\n",
      "                 trash         17         17      0.618      0.824      0.831      0.768\n",
      "                 vinyl         26         26      0.358      0.731       0.61      0.528\n",
      "Speed: 0.2ms preprocess, 21.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\jk316\\project2\\runs\\detect\\waste_classification\u001b[0m\n",
      "\n",
      "훈련 완료!\n",
      "훈련 시간: 4시간 0분 58초\n",
      "결과 저장: runs/detect/waste_classification\n",
      "최적 모델을 'best_waste_model.pt'로 복사했습니다.\n",
      "\n",
      "3단계 완료!\n",
      "다음: python step4_folder_test.py 실행\n"
     ]
    }
   ],
   "source": [
    "# 3단계: 모델 훈련\n",
    "class WasteModelTrainer:\n",
    "    def __init__(self, dataset_yaml=\"dataset.yaml\"):\n",
    "        self.dataset_yaml = dataset_yaml\n",
    "        self.device = 'cpu'\n",
    "        self.results_path = Path(\"training_results\")\n",
    "        self.results_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(\"YOLO 모델 훈련 준비\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "    \n",
    "    def check_dataset(self):\n",
    "        print(\"\\n데이터셋 상태 확인 중...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not Path(self.dataset_yaml).exists():\n",
    "            print(f\"{self.dataset_yaml} 파일이 없습니다!\")\n",
    "            return False\n",
    "        \n",
    "        with open(self.dataset_yaml, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        dataset_path = Path(config['path'])\n",
    "        train_images = dataset_path / config['train']\n",
    "        val_images = dataset_path / config['val']\n",
    "        train_labels = dataset_path / \"labels\" / \"train\"\n",
    "        val_labels = dataset_path / \"labels\" / \"val\"\n",
    "        \n",
    "        folders_ok = True\n",
    "        for folder, name in [(train_images, \"훈련 이미지\"), (val_images, \"검증 이미지\"), \n",
    "                            (train_labels, \"훈련 라벨\"), (val_labels, \"검증 라벨\")]:\n",
    "            if folder.exists():\n",
    "                count = len(list(folder.glob(\"*\")))\n",
    "                print(f\"{name}: {count}개\")\n",
    "            else:\n",
    "                print(f\"{name}: 폴더 없음\")\n",
    "                folders_ok = False\n",
    "        \n",
    "        print(f\"클래스 수: {config['nc']}\")\n",
    "        print(f\"클래스: {config['names']}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not folders_ok:\n",
    "            print(\"필요한 폴더가 없습니다. 2단계를 먼저 완료해주세요.\")\n",
    "            return False\n",
    "        \n",
    "        print(\"데이터셋 준비 완료!\")\n",
    "        return True\n",
    "    \n",
    "    def train_model(self, model_size='n', epochs=50, batch_size=8, img_size=640):\n",
    "        print(f\"\\n모델 훈련 시작!\")\n",
    "        print(f\"모델 크기: YOLOv8{model_size}\")\n",
    "        print(f\"에포크: {epochs}\")\n",
    "        print(f\"배치 크기: {batch_size}\")\n",
    "        print(f\"이미지 크기: {img_size}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        model = YOLO(f'yolov8{model_size}.pt')\n",
    "        print(f\"YOLOv8{model_size} 사전훈련 모델 로드 완료\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            results = model.train(\n",
    "                data=self.dataset_yaml,\n",
    "                epochs=epochs,\n",
    "                imgsz=img_size,\n",
    "                batch=batch_size,\n",
    "                device=self.device,\n",
    "                project='runs/detect',\n",
    "                name='waste_classification',\n",
    "                save=True,\n",
    "                plots=True,\n",
    "                patience=10,\n",
    "                save_period=10,\n",
    "                workers=2,\n",
    "                verbose=True,\n",
    "                val=True,\n",
    "                cache=False,\n",
    "                amp=False,\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            hours = int(training_time // 3600)\n",
    "            minutes = int((training_time % 3600) // 60)\n",
    "            seconds = int(training_time % 60)\n",
    "            \n",
    "            print(f\"\\n훈련 완료!\")\n",
    "            print(f\"훈련 시간: {hours}시간 {minutes}분 {seconds}초\")\n",
    "            print(f\"결과 저장: runs/detect/waste_classification\")\n",
    "            \n",
    "            best_model_path = Path(\"runs/detect/waste_classification/weights/best.pt\")\n",
    "            if best_model_path.exists():\n",
    "                shutil.copy2(best_model_path, \"best_waste_model.pt\")\n",
    "                print(\"최적 모델을 'best_waste_model.pt'로 복사했습니다.\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"훈련 중 오류 발생: {e}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    print(\"3단계: YOLO 쓰레기 분류 모델 훈련\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    trainer = WasteModelTrainer()\n",
    "    \n",
    "    if not trainer.check_dataset():\n",
    "        print(\"\\n데이터셋 문제가 있습니다. 2단계를 먼저 완료해주세요.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n빠른 훈련 설정 (30 에포크)\")\n",
    "    trainer.train_model(model_size='n', epochs=30, batch_size=4, img_size=416)\n",
    "    \n",
    "    print(\"\\n3단계 완료!\")\n",
    "    print(\"다음: python step4_folder_test.py 실행\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e652b2",
   "metadata": {},
   "source": [
    "### 4단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e9bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료\n",
      "\n",
      "'./test_images' 폴더에 테스트 이미지를 넣어주세요\n",
      "\n",
      "6개 이미지 분석 시작...\n",
      "\n",
      "분석 중: paper491.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\jk316\\OneDrive\\Desktop\\practice4\\test_images\\paper491.jpg: 320x416 1 trash, 45.2ms\n",
      "Speed: 1.7ms preprocess, 45.2ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 416)\n",
      "  발견: trash (0.82)\n",
      "\n",
      "분석 중: paper559.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\jk316\\OneDrive\\Desktop\\practice4\\test_images\\paper559.jpg: 320x416 1 plastic, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 416)\n",
      "  발견: plastic (0.8)\n",
      "\n",
      "분석 중: plastic171.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\jk316\\OneDrive\\Desktop\\practice4\\test_images\\plastic171.jpg: 320x416 1 vinyl, 36.5ms\n",
      "Speed: 0.6ms preprocess, 36.5ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 416)\n",
      "  발견: vinyl (0.79)\n",
      "\n",
      "분석 중: plastic313.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\jk316\\OneDrive\\Desktop\\practice4\\test_images\\plastic313.jpg: 320x416 1 vinyl, 36.4ms\n",
      "Speed: 0.8ms preprocess, 36.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 416)\n",
      "  발견: vinyl (0.98)\n",
      "\n",
      "분석 중: trash5.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\jk316\\OneDrive\\Desktop\\practice4\\test_images\\trash5.jpg: 416x384 1 plastic, 45.3ms\n",
      "Speed: 1.2ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 384)\n",
      "  발견: plastic (0.69)\n",
      "\n",
      "분석 중: vinyl96.png\n",
      "\n",
      "image 1/1 c:\\Users\\jk316\\OneDrive\\Desktop\\practice4\\test_images\\vinyl96.png: 416x416 1 vinyl, 44.6ms\n",
      "Speed: 0.9ms preprocess, 44.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "  발견: vinyl (0.99)\n",
      "\n",
      "완료! 결과: ./results\n"
     ]
    }
   ],
   "source": [
    "# 4단계: 저장된 이미지 분석\n",
    "\n",
    "class WasteImageAnalyzer:\n",
    "    def __init__(self, model_path=\"best_waste_model.pt\"):\n",
    "        if not Path(model_path).exists():\n",
    "            print(f\"모델 파일이 없습니다: {model_path}\")\n",
    "            print(\"3단계를 먼저 완료하세요\")\n",
    "            return\n",
    "        \n",
    "        self.model = YOLO(model_path)\n",
    "        print(\"모델 로드 완료\")\n",
    "    \n",
    "    def analyze_folder(self, input_folder, output_folder):\n",
    "        input_path = Path(input_folder)\n",
    "        output_path = Path(output_folder)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        images = list(input_path.glob('*.jpg'))\n",
    "        images.extend(list(input_path.glob('*.jpeg')))\n",
    "        images.extend(list(input_path.glob('*.png')))\n",
    "        \n",
    "        if not images:\n",
    "            print(f\"'{input_folder}'에 이미지가 없습니다\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{len(images)}개 이미지 분석 시작...\")\n",
    "        \n",
    "        for img_path in images:\n",
    "            print(f\"\\n분석 중: {img_path.name}\")\n",
    "            \n",
    "            results = self.model(str(img_path), conf=0.3)\n",
    "            \n",
    "            detections = []\n",
    "            for box in results[0].boxes:\n",
    "                detections.append({\n",
    "                    'class': self.model.names[int(box.cls[0])],\n",
    "                    'confidence': round(float(box.conf[0]), 2),\n",
    "                    'bbox': [int(x) for x in box.xyxy[0].tolist()]\n",
    "                })\n",
    "            \n",
    "            if detections:\n",
    "                for det in detections:\n",
    "                    print(f\"  발견: {det['class']} ({det['confidence']})\")\n",
    "            else:\n",
    "                print(\"  탐지된 객체 없음\")\n",
    "            \n",
    "            annotated = results[0].plot()\n",
    "            result_img_path = output_path / f\"{img_path.stem}_result.jpg\"\n",
    "            cv.imwrite(str(result_img_path), annotated)\n",
    "            \n",
    "            result_json = {\n",
    "                'original': str(img_path),\n",
    "                'result_image': str(result_img_path),\n",
    "                'detections': detections,\n",
    "                'count': len(detections)\n",
    "            }\n",
    "            \n",
    "            json_path = output_path / f\"{img_path.stem}_result.json\"\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result_json, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n완료! 결과: {output_folder}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = WasteImageAnalyzer()\n",
    "    \n",
    "    input_folder = input(\"분석할 이미지 폴더 경로 (기본: ./test_images): \").strip() or \"./test_images\"\n",
    "    output_folder = input(\"결과 저장 폴더 경로 (기본: ./results): \").strip() or \"./results\"\n",
    "    \n",
    "    Path(input_folder).mkdir(exist_ok=True)\n",
    "    print(f\"\\n'{input_folder}' 폴더에 테스트 이미지를 넣어주세요\")\n",
    "    \n",
    "    input(\"준비되면 Enter를 누르세요...\")\n",
    "    \n",
    "    analyzer.analyze_folder(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pracice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
